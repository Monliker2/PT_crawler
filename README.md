# Web File Link Crawler

Этот скрипт на Python рекурсивно обходит веб‑сайт, начиная с заданного URL, и останавливается при первом обнаружении прямой ссылки на файл. Поддерживается анализ HTML и встроенного JavaScript, а также строгое (whitelist) и гибкое определение файловых ссылок.

## Возможности

* Обход сайта в ширину (BFS) с очередью и переходом на родительские URL при необходимости.
* Поддержка строгого (`strict=True`) и негибкого (`strict=False`) режимов определения ссылок на файлы.
* Выделение ссылок из тегов `<a>`, `<link>`, `<button>`, а также из атрибутов `onclick` и `data-url`.
* Парсинг URL из JavaScript-кода на странице.
* Логгирование каждого шага с модулем `logging` (уровень DEBUG).
* Ограничение общего времени сканирования (по умолчанию 30 секунд).
* Остановка при первом найденном файле.

## Требования

* Python 3.7+
  
## Установка

Склонируйте репозиторий или скопируйте скрипт `crawler.py` в ваш проект.

```bash
pip install -r requirements.txt
```

## Структура скрипта

* **`get_parent_url(url)`**
  Возвращает URL родительской директории.

* **`is_file_link(response, url, strict=True)`**
  Определяет, является ли URL ссылкой на файл:

  * По заголовку `Content-Disposition: attachment`.
  * По MIME‑типу и расширению (whitelist в строгом режиме).

* **`parse_html_for_links(soup, base_url, visited, start_url, queue)`**
  Извлекает ссылки из HTML‑документа (теги `<a>`, `<link>`, `<button>`), проверяет их и кладёт в очередь.

* **`extract_href_from_tag(tag)`**
  Извлекает значение ссылки из атрибутов `href`, `onclick` и `data-url`.

* **`is_internal_link(start_url, abs_url)`**
  Проверяет, что домен ссылки совпадает с доменом начального URL.

* **`parse_js_for_links(soup, base_url, visited, queue)`**
  Ищет URL внутри `<script>`-тегов.

* **`scan(start_url, timeout=30)`**
  Основная функция сканирования:

  1. Помещает `start_url` в очередь.
  2. Пока очередь не пуста и таймаут не истёк:

     * Берёт следующий URL, делает HTTP-запрос.
     * Проверяет, является ли это файлом (`is_file_link`).
     * Если HTML, парсит ссылки и JS.
     * Помещает новые ссылки в очередь.
     * Останавливается при нахождении файла.

## Использование

```bash
python crawler.py <URL> [timeout_in_seconds]
```

* `<URL>` — начальная страница для сканирования.
* `[timeout_in_seconds]` — (необязательно) максимальное время работы сканера (по умолчанию 30).

### Пример

```bash
python crawler.py https://example.com/files 20
```

**Вывод будет содержать** информационные сообщения уровня INFO или DEBUG (в зависимости от настройки `logging`), например:

```
2025-05-29 17:00:00 - INFO - Обработка URL: https://example.com/files
2025-05-29 17:00:01 - INFO - Найдена ссылка на файл: https://example.com/files/report.pdf
```
